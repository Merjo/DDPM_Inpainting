#######################################################################################
### conda added to PATH. Use 'source activate' rather than 'conda activate'.        ###
### Do not run 'conda init' if prompted.                                            ###
### 'conda init' will modify your shell outside of the `module` environment         ###
#######################################################################################
[I 2025-10-27 14:59:33,453] A new study created in RDB with name: rain_diffusion_1761573572
[I 2025-10-27 15:35:03,110] Trial 0 finished with value: 0.02280703124459833 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.11035084470740479, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'quadratic', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 3.5136193561568124e-05}. Best is trial 0 with value: 0.02280703124459833.
[I 2025-10-27 15:49:44,602] Trial 1 finished with value: 0.02513715526955202 and parameters: {'model_channels': 128, 'num_blocks': 1, 'dropout': 0.12412249179914786, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last', 'timesteps': 250, 'beta_schedule': 'cosine', 'loss': 'l1', 'optimizer': 'Adam', 'scheduler': 'WarmupCosine', 'lr': 2.4420255283405248e-05}. Best is trial 0 with value: 0.02280703124459833.
[I 2025-10-27 16:11:34,109] Trial 2 finished with value: 0.03422726968973875 and parameters: {'model_channels': 128, 'num_blocks': 2, 'dropout': 0.0031038210295324053, 'downsample_type': 'residual', 'channel_mult': '1248', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'exponential', 'loss': 'mse', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 3.135571356927153e-05}. Best is trial 0 with value: 0.02280703124459833.
[I 2025-10-27 16:25:24,077] Trial 3 finished with value: 0.042204452186822894 and parameters: {'model_channels': 128, 'num_blocks': 1, 'dropout': 0.27510541244051284, 'downsample_type': 'residual', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 250, 'beta_schedule': 'quadratic', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 9.380400810987425e-05}. Best is trial 0 with value: 0.02280703124459833.
[I 2025-10-27 16:46:18,162] Trial 4 finished with value: 0.04109877256117761 and parameters: {'model_channels': 64, 'num_blocks': 3, 'dropout': 0.08083819129906769, 'downsample_type': 'residual', 'channel_mult': '1224', 'attn_config': 'none', 'timesteps': 250, 'beta_schedule': 'quadratic', 'loss': 'mse', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 2.0188290404833445e-05}. Best is trial 0 with value: 0.02280703124459833.
[I 2025-10-27 16:58:10,645] Trial 5 finished with value: 0.022435764519684015 and parameters: {'model_channels': 128, 'num_blocks': 2, 'dropout': 0.02284747072253941, 'downsample_type': 'residual', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'quadratic', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 1.7517708761838826e-05}. Best is trial 5 with value: 0.022435764519684015.
[I 2025-10-27 17:01:06,983] Trial 6 pruned. 
[I 2025-10-27 17:07:53,174] Trial 7 pruned. 
[I 2025-10-27 17:22:26,216] Trial 8 finished with value: 0.02563734012860805 and parameters: {'model_channels': 128, 'num_blocks': 1, 'dropout': 0.26654205223242255, 'downsample_type': 'residual', 'channel_mult': '1124', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'cosine', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'WarmupCosine', 'lr': 7.230754444018545e-05}. Best is trial 5 with value: 0.022435764519684015.
[I 2025-10-27 17:24:40,577] Trial 9 pruned. 
[I 2025-10-27 17:26:24,681] Trial 10 pruned. 
[I 2025-10-27 18:01:47,438] Trial 11 finished with value: 0.022195766873750834 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.05265595127246129, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'quadratic', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 3.863319285735632e-05}. Best is trial 11 with value: 0.022195766873750834.
[I 2025-10-27 18:38:52,671] Trial 12 finished with value: 0.021927740276884287 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.05459915678525068, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'quadratic', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 4.8208208767361915e-05}. Best is trial 12 with value: 0.021927740276884287.
[I 2025-10-27 19:08:18,264] Trial 13 finished with value: 0.018959858970716597 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.05989518244176899, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 4.714002005800465e-05}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-27 19:26:48,309] Trial 14 finished with value: 0.019684583785012363 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.06835228232913523, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.0001266593061916152}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-27 20:02:35,415] Trial 15 finished with value: 0.019530423972848803 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.21477582073909235, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.0001337616776939955}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-27 20:38:22,268] Trial 16 finished with value: 0.01904835548568517 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.22022031566190653, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.0001733388028785504}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-27 21:14:11,903] Trial 17 finished with value: 0.019506614553183317 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.2272735374143947, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.00023032448988066092}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-27 21:55:34,505] Trial 18 finished with value: 0.019391059975884854 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.1541014412061285, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'last', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.000216823476725978}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-27 22:14:33,970] Trial 19 finished with value: 0.01936759824026376 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.24216379287888945, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'huber', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.00043133347694608007}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-27 22:50:19,122] Trial 20 finished with value: 0.019307053164578973 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.29865718896859467, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 5.67403218374444e-05}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-27 23:26:08,761] Trial 21 finished with value: 0.019038318023644387 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.20013316640138173, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 6.085778888794657e-05}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-28 00:02:01,954] Trial 22 finished with value: 0.01936398705523461 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.18910126429394802, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 9.375858214396519e-05}. Best is trial 13 with value: 0.018959858970716597.
[I 2025-10-28 00:07:20,816] Trial 23 pruned. 
[I 2025-10-28 00:36:45,682] Trial 24 finished with value: 0.018884148810058832 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.1353065453500296, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.00011528253541556773}. Best is trial 24 with value: 0.018884148810058832.
[I 2025-10-28 00:38:11,024] Trial 25 pruned. 
[I 2025-10-28 01:07:33,331] Trial 26 finished with value: 0.018587911287508904 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.09078637595546848, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'huber', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 9.227458625265588e-05}. Best is trial 26 with value: 0.018587911287508904.
[I 2025-10-28 01:22:58,270] Trial 27 finished with value: 0.01903799226656556 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.08681044175975186, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'huber', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 9.34246215501653e-05}. Best is trial 26 with value: 0.018587911287508904.
[I 2025-10-28 01:32:39,407] Trial 28 pruned. 
[I 2025-10-28 01:37:18,262] Trial 29 pruned. 
[I 2025-10-28 01:38:43,564] Trial 30 pruned. 
[I 2025-10-28 02:05:13,595] Trial 31 finished with value: 0.020165802760794757 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.08684167271894272, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'huber', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 9.433062557030866e-05}. Best is trial 26 with value: 0.018587911287508904.
[I 2025-10-28 02:31:48,827] Trial 32 finished with value: 0.01915688829915598 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.06640676192484236, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'huber', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.00012577019306232902}. Best is trial 26 with value: 0.018587911287508904.
[I 2025-10-28 02:53:48,964] Trial 33 finished with value: 0.019083370107412338 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.13393830396498732, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'huber', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 8.380562884734386e-05}. Best is trial 26 with value: 0.018587911287508904.
[I 2025-10-28 02:56:00,965] Trial 34 pruned. 
[I 2025-10-28 02:58:15,300] Trial 35 pruned. 
[I 2025-10-28 02:59:43,858] Trial 36 pruned. 
[I 2025-10-28 03:02:24,411] Trial 37 pruned. 
[I 2025-10-28 03:04:08,730] Trial 38 pruned. 
[I 2025-10-28 03:05:38,223] Trial 39 pruned. 
[I 2025-10-28 03:08:16,299] Trial 40 pruned. 
[I 2025-10-28 03:10:27,475] Trial 41 pruned. 
[I 2025-10-28 03:12:38,712] Trial 42 pruned. 
[I 2025-10-28 03:32:07,996] Trial 43 finished with value: 0.01904900616062805 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.050599253621901034, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 8.623206358739381e-05}. Best is trial 26 with value: 0.018587911287508904.
[I 2025-10-28 03:34:19,511] Trial 44 pruned. 
[I 2025-10-28 03:36:39,819] Trial 45 pruned. 
[I 2025-10-28 03:57:02,760] Trial 46 finished with value: 0.013303605898702517 and parameters: {'model_channels': 128, 'num_blocks': 2, 'dropout': 0.06863037218446748, 'downsample_type': 'residual', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.0001535124882349191}. Best is trial 46 with value: 0.013303605898702517.
[I 2025-10-28 03:58:31,728] Trial 47 pruned. 
[I 2025-10-28 04:07:18,441] Trial 48 finished with value: 0.01362222025739029 and parameters: {'model_channels': 128, 'num_blocks': 2, 'dropout': 0.01433311466294171, 'downsample_type': 'residual', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00014863196547450355}. Best is trial 46 with value: 0.013303605898702517.
[I 2025-10-28 04:08:46,179] Trial 49 pruned. 
/home/merlinho/testspace/DDPM_Inpainting/src/run/run_optuna.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  best_model = torch.load(best_trial.user_attrs["checkpoint"])
Traceback (most recent call last):
  File "/home/merlinho/testspace/DDPM_Inpainting/src/run/run_best_optuna.py", line 94, in <module>
    diffusion, unet, best_loss, params = run_best_optuna(n_trials=n_trials,
  File "/home/merlinho/testspace/DDPM_Inpainting/src/run/run_best_optuna.py", line 78, in run_best_optuna
    diffusion, unet, best_loss, params = run_best(param_file=param_filename,
  File "/home/merlinho/testspace/DDPM_Inpainting/src/run/run_best.py", line 146, in run_best
    diffusion, unet, best_loss, params = train_best_model(param_file=param_file,
  File "/home/merlinho/testspace/DDPM_Inpainting/src/run/run_best.py", line 85, in train_best_model
    best_loss, epoch_losses = diffusion.train(loader, optimizer, epochs=epochs, scheduler=scheduler, patience=patience, log_every_epoch=True, sample_every=cfg.sample_every)
  File "/home/merlinho/testspace/DDPM_Inpainting/src/model/diffusion.py", line 186, in train
    if trial.should_prune():
  File "/home/merlinho/.conda/envs/diffusion_rain/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/merlinho/testspace/DDPM_Inpainting/src/model/diffusion.py", line 197, in plot_samples
    return best_rmse, epoch_losses
TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'
