O#######################################################################################
### conda added to PATH. Use 'source activate' rather than 'conda activate'.        ###
### Do not run 'conda init' if prompted.                                            ###
### 'conda init' will modify your shell outside of the `module` environment         ###
#######################################################################################
[I 2025-10-25 00:16:18,682] A new study created in RDB with name: rain_diffusion_1761344178
[I 2025-10-25 00:23:22,449] Trial 0 finished with value: 0.014437418635979223 and parameters: {'model_channels': 128, 'num_blocks': 3, 'dropout': 0.19967342037261018, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00010066659677583863}. Best is trial 0 with value: 0.014437418635979223.
slurmstepd-csl14c257: error: _handle_stat_jobacct: Took usec=-7578696, which is more than MessageTimeout (10s). The result won't be delivered
[I 2025-10-25 00:48:34,069] Trial 1 finished with value: 0.026608162471792474 and parameters: {'model_channels': 256, 'num_blocks': 3, 'dropout': 0.02830356967323515, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 250, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'WarmupCosine', 'lr': 0.00012523859915414772}. Best is trial 0 with value: 0.014437418635979223.
[I 2025-10-25 00:56:54,254] Trial 2 finished with value: 0.026371456575565062 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.10500848069581749, 'downsample_type': 'standard', 'channel_mult': '1248', 'attn_config': 'last', 'timesteps': 500, 'beta_schedule': 'cosine', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00022736365579036793}. Best is trial 0 with value: 0.014437418635979223.
[I 2025-10-25 01:03:07,819] Trial 3 finished with value: 0.0408525321388635 and parameters: {'model_channels': 64, 'num_blocks': 1, 'dropout': 0.11610639756046598, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'exponential', 'loss': 'huber', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 1.4913603634702872e-05}. Best is trial 0 with value: 0.014437418635979223.
[I 2025-10-25 01:10:50,211] Trial 4 finished with value: 0.042768771992252465 and parameters: {'model_channels': 128, 'num_blocks': 3, 'dropout': 0.22681674637378443, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 250, 'beta_schedule': 'quadratic', 'loss': 'huber', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.00012841036906552437}. Best is trial 0 with value: 0.014437418635979223.
[I 2025-10-25 01:11:29,954] Trial 5 pruned. 
[I 2025-10-25 01:18:06,998] Trial 6 pruned. 
[I 2025-10-25 01:20:14,769] Trial 7 pruned. 
[I 2025-10-25 01:23:36,276] Trial 8 pruned. 
[I 2025-10-25 01:30:53,959] Trial 9 finished with value: 0.020819015664081224 and parameters: {'model_channels': 64, 'num_blocks': 3, 'dropout': 0.2214116324635826, 'downsample_type': 'residual', 'channel_mult': '1248', 'attn_config': 'last_two', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'huber', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00027237780109162893}. Best is trial 0 with value: 0.014437418635979223.
[I 2025-10-25 01:33:30,657] Trial 10 pruned. 
[I 2025-10-25 01:34:36,738] Trial 11 pruned. 
[I 2025-10-25 01:36:27,799] Trial 12 pruned. 
[I 2025-10-25 01:44:47,437] Trial 13 finished with value: 0.0206998895002773 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.29059293552237064, 'downsample_type': 'residual', 'channel_mult': '1248', 'attn_config': 'last_two', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00031265977887901393}. Best is trial 0 with value: 0.014437418635979223.
[I 2025-10-25 01:54:16,122] Trial 14 finished with value: 0.015425990716503642 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.29569642969828647, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00039172083171404044}. Best is trial 0 with value: 0.014437418635979223.
[I 2025-10-25 02:05:18,858] Trial 15 finished with value: 0.014322876188600548 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.26692949002911515, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.0004774169173962966}. Best is trial 15 with value: 0.014322876188600548.
[I 2025-10-25 02:06:32,244] Trial 16 pruned. 
[I 2025-10-25 02:09:41,470] Trial 17 pruned. 
[I 2025-10-25 02:11:13,227] Trial 18 pruned. 
[I 2025-10-25 02:29:01,194] Trial 19 finished with value: 0.01428874201861362 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.1554621264629931, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00018064296757824163}. Best is trial 19 with value: 0.01428874201861362.
[I 2025-10-25 02:44:49,936] Trial 20 finished with value: 0.013820020467252694 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.002855459873455979, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00018712780137181376}. Best is trial 20 with value: 0.013820020467252694.
[I 2025-10-25 02:52:47,648] Trial 21 finished with value: 0.015320812555077596 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.020949313778116062, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.0001862707922840216}. Best is trial 20 with value: 0.013820020467252694.
[I 2025-10-25 03:00:45,393] Trial 22 finished with value: 0.015802876717342547 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.06393491584234437, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.0001777944264276309}. Best is trial 20 with value: 0.013820020467252694.
[I 2025-10-25 03:18:32,160] Trial 23 finished with value: 0.014414362555827957 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.060084360001425155, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.0001638206298649731}. Best is trial 20 with value: 0.013820020467252694.
[I 2025-10-25 03:36:18,860] Trial 24 finished with value: 0.01401976047503277 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.15092323875904692, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00036689955743659234}. Best is trial 20 with value: 0.013820020467252694.
[I 2025-10-25 03:38:15,683] Trial 25 pruned. 
[I 2025-10-25 03:39:47,112] Trial 26 pruned. 
[I 2025-10-25 03:41:46,956] Trial 27 pruned. 
[I 2025-10-25 03:43:18,326] Trial 28 pruned. 
[I 2025-10-25 03:47:15,736] Trial 29 pruned. 
[I 2025-10-25 03:49:32,299] Trial 30 pruned. 
[I 2025-10-25 04:01:25,498] Trial 31 finished with value: 0.01580370524555992 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.1369416844870251, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.0003683705839708001}. Best is trial 20 with value: 0.013820020467252694.
[I 2025-10-25 04:05:23,539] Trial 32 pruned. 
[I 2025-10-25 04:08:04,308] Trial 33 pruned. 
[I 2025-10-25 04:16:01,126] Trial 34 finished with value: 0.015131440702840662 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.0328463922296377, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.000438328516585772}. Best is trial 20 with value: 0.013820020467252694.
[I 2025-10-25 04:29:51,936] Trial 35 finished with value: 0.014602107733425598 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.2711219420685774, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.00022063297773329446}. Best is trial 20 with value: 0.013820020467252694.
[I 2025-10-25 04:32:33,370] Trial 36 pruned. 
[I 2025-10-25 04:34:29,870] Trial 37 pruned. 
[I 2025-10-25 04:35:42,745] Trial 38 pruned. 
[I 2025-10-25 04:36:36,337] Trial 39 pruned. 
[I 2025-10-25 04:38:22,888] Trial 40 pruned. 
[I 2025-10-25 04:56:08,902] Trial 41 finished with value: 0.013646426079155038 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.0015810361283177601, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.000179013101914261}. Best is trial 41 with value: 0.013646426079155038.
[I 2025-10-25 05:02:04,203] Trial 42 pruned. 
[I 2025-10-25 05:15:50,028] Trial 43 pruned. 
[I 2025-10-25 05:27:54,463] Trial 44 finished with value: 0.014624656925086504 and parameters: {'model_channels': 256, 'num_blocks': 2, 'dropout': 0.03422883260823988, 'downsample_type': 'residual', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.0003240512648506887}. Best is trial 41 with value: 0.013646426079155038.
[I 2025-10-25 05:29:13,706] Trial 45 pruned. 
[I 2025-10-25 05:30:01,686] Trial 46 pruned. 
[I 2025-10-25 05:36:52,288] Trial 47 pruned. 
[I 2025-10-25 05:38:04,109] Trial 48 pruned. 
[I 2025-10-25 05:39:35,263] Trial 49 pruned. 
/home/merlinho/testspace/DDPM_Inpainting/src/run/run_optuna.py:147: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  best_model = torch.load(best_trial.user_attrs["checkpoint"])
Traceback (most recent call last):
  File "/home/merlinho/testspace/DDPM_Inpainting/src/run/run_best_optuna.py", line 95, in <module>
    diffusion, unet, best_loss, params = run_best_optuna(n_trials=n_trials,
  File "/home/merlinho/testspace/DDPM_Inpainting/src/run/run_best_optuna.py", line 79, in run_best_optuna
    diffusion, unet, best_loss, params = run_best(param_file=param_filename,
  File "/home/merlinho/testspace/DDPM_Inpainting/src/run/run_best.py", line 148, in run_best
    diffusion, unet, best_loss, params = train_best_model(param_file=param_file,
  File "/home/merlinho/testspace/DDPM_Inpainting/src/run/run_best.py", line 54, in train_best_model
    unet.load_state_dict(torch.load(model_file, map_location=device, weights_only=True))
  File "/home/merlinho/.conda/envs/diffusion_rain/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for SongUNet:
	Missing key(s) in state_dict: "enc.32x32_block0.norm2.weight", "enc.32x32_block0.norm2.bias", "enc.32x32_block0.qkv.weight", "enc.32x32_block0.qkv.bias", "enc.32x32_block0.proj.weight", "enc.32x32_block0.proj.bias", "enc.32x32_block1.norm2.weight", "enc.32x32_block1.norm2.bias", "enc.32x32_block1.qkv.weight", "enc.32x32_block1.qkv.bias", "enc.32x32_block1.proj.weight", "enc.32x32_block1.proj.bias", "dec.32x32_block2.norm2.weight", "dec.32x32_block2.norm2.bias", "dec.32x32_block2.qkv.weight", "dec.32x32_block2.qkv.bias", "dec.32x32_block2.proj.weight", "dec.32x32_block2.proj.bias". 
	Unexpected key(s) in state_dict: "enc.16x16_block0.norm2.weight", "enc.16x16_block0.norm2.bias", "enc.16x16_block0.qkv.weight", "enc.16x16_block0.qkv.bias", "enc.16x16_block0.proj.weight", "enc.16x16_block0.proj.bias", "enc.16x16_block1.norm2.weight", "enc.16x16_block1.norm2.bias", "enc.16x16_block1.qkv.weight", "enc.16x16_block1.qkv.bias", "enc.16x16_block1.proj.weight", "enc.16x16_block1.proj.bias", "dec.16x16_block2.norm2.weight", "dec.16x16_block2.norm2.bias", "dec.16x16_block2.qkv.weight", "dec.16x16_block2.qkv.bias", "dec.16x16_block2.proj.weight", "dec.16x16_block2.proj.bias". 
