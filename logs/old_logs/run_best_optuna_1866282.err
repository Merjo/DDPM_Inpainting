#######################################################################################
### conda added to PATH. Use 'source activate' rather than 'conda activate'.        ###
### Do not run 'conda init' if prompted.                                            ###
### 'conda init' will modify your shell outside of the `module` environment         ###
#######################################################################################
[I 2025-09-19 13:45:15,507] A new study created in memory with name: no-name-730bbab8-212c-4592-b3e6-9a8d78a5d5b2
[I 2025-09-19 14:11:36,195] Trial 0 finished with value: 0.03249819383089399 and parameters: {'model_channels': 64, 'num_blocks': 1, 'dropout': 0.18475404519333546, 'downsample_type': 'residual', 'channel_mult': '124', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'cosine', 'loss': 'l1', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 6.686756217930708e-05}. Best is trial 0 with value: 0.03249819383089399.
[I 2025-09-19 16:18:06,208] Trial 1 finished with value: 0.04128657557423516 and parameters: {'model_channels': 128, 'num_blocks': 2, 'dropout': 0.17884085819819062, 'downsample_type': 'residual', 'channel_mult': '124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'exponential', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.000460309890522064}. Best is trial 0 with value: 0.03249819383089399.
[I 2025-09-19 16:50:58,454] Trial 2 finished with value: 0.07296567135972196 and parameters: {'model_channels': 128, 'num_blocks': 3, 'dropout': 0.00444990469805584, 'downsample_type': 'residual', 'channel_mult': '1124', 'attn_config': 'none', 'timesteps': 250, 'beta_schedule': 'exponential', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0009722164172517492}. Best is trial 0 with value: 0.03249819383089399.
[I 2025-09-19 17:07:09,609] Trial 3 finished with value: 0.0332178586628288 and parameters: {'model_channels': 32, 'num_blocks': 2, 'dropout': 0.25163107576950094, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 500, 'beta_schedule': 'cosine', 'loss': 'l1', 'optimizer': 'Adam', 'scheduler': 'ExponentialLR', 'lr': 0.0001754863408832002}. Best is trial 0 with value: 0.03249819383089399.
[I 2025-09-19 17:28:24,159] Trial 4 finished with value: 0.017648353483849046 and parameters: {'model_channels': 32, 'num_blocks': 3, 'dropout': 0.2390716178155717, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.00024988333984943403}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 17:32:01,546] Trial 5 pruned. 
[I 2025-09-19 17:33:24,244] Trial 6 pruned. 
[I 2025-09-19 17:49:20,508] Trial 7 pruned. 
[I 2025-09-19 17:51:52,454] Trial 8 pruned. 
[I 2025-09-19 18:43:54,571] Trial 9 finished with value: 0.0318899176806083 and parameters: {'model_channels': 128, 'num_blocks': 3, 'dropout': 0.2222096786013568, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'last', 'timesteps': 250, 'beta_schedule': 'cosine', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'WarmupCosine', 'lr': 0.0009994190294173894}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 18:46:33,998] Trial 10 pruned. 
[I 2025-09-19 19:07:49,790] Trial 11 pruned. 
[I 2025-09-19 19:29:35,707] Trial 12 finished with value: 0.02695642422065781 and parameters: {'model_channels': 32, 'num_blocks': 3, 'dropout': 0.28090149321306584, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'quadratic', 'loss': 'mse', 'optimizer': 'Adam', 'scheduler': 'WarmupCosine', 'lr': 0.0009777080466362074}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 19:32:19,086] Trial 13 pruned. 
[I 2025-09-19 19:49:08,558] Trial 14 finished with value: 0.028770253117110622 and parameters: {'model_channels': 32, 'num_blocks': 2, 'dropout': 0.2699356581046718, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'none', 'timesteps': 1000, 'beta_schedule': 'quadratic', 'loss': 'l1', 'optimizer': 'Adam', 'scheduler': 'WarmupCosine', 'lr': 0.0007537632889281504}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 19:51:47,841] Trial 15 pruned. 
[I 2025-09-19 20:07:29,649] Trial 16 finished with value: 0.019195868845108426 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.26311407142012483, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0022631562484415188}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 20:23:11,539] Trial 17 finished with value: 0.019047963756923404 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.1584817911079131, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.004297651296781375}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 20:27:06,647] Trial 18 pruned. 
[I 2025-09-19 20:43:32,318] Trial 19 finished with value: 0.02464294429271664 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.15763299258531946, 'downsample_type': 'standard', 'channel_mult': '1248', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.00472428929167892}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 21:06:17,980] Trial 20 finished with value: 0.024017985564142724 and parameters: {'model_channels': 64, 'num_blocks': 1, 'dropout': 0.0954889281269595, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'none', 'timesteps': 500, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0019212057912947964}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 21:21:59,555] Trial 21 finished with value: 0.01892950373712456 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.20750658914197725, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.002398176215112495}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 21:37:41,242] Trial 22 finished with value: 0.01884320273717073 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.203430303737178, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0029217321305166095}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 21:53:22,852] Trial 23 finished with value: 0.018576767386456458 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.20032131733734732, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0016724294426065714}. Best is trial 4 with value: 0.017648353483849046.
[I 2025-09-19 22:24:45,588] Trial 24 finished with value: 0.017079674258651534 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.20971858911878616, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0005829290468170934}. Best is trial 24 with value: 0.017079674258651534.
[I 2025-09-19 22:56:08,026] Trial 25 finished with value: 0.016573251293832972 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.24533573086065602, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0005525659953081826}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-19 23:19:20,728] Trial 26 finished with value: 0.017539934740380396 and parameters: {'model_channels': 64, 'num_blocks': 1, 'dropout': 0.2532989752079669, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0005296593927294004}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-19 23:49:51,232] Trial 27 finished with value: 0.017699002497963308 and parameters: {'model_channels': 64, 'num_blocks': 1, 'dropout': 0.2611047694435142, 'downsample_type': 'residual', 'channel_mult': '1248', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'ExponentialLR', 'lr': 0.00020561574967109427}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-19 23:52:12,259] Trial 28 pruned. 
[I 2025-09-19 23:55:49,341] Trial 29 pruned. 
[I 2025-09-19 23:58:43,405] Trial 30 pruned. 
[I 2025-09-20 00:01:34,191] Trial 31 pruned. 
[I 2025-09-20 00:35:08,717] Trial 32 finished with value: 0.017056667896645977 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.27378269820989526, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0005482266807381381}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 01:08:43,470] Trial 33 finished with value: 0.017222657098418807 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.2815414648358527, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0006917197079595234}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 01:13:00,062] Trial 34 pruned. 
[I 2025-09-20 01:41:07,487] Trial 35 finished with value: 0.017041650594040836 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.2999124814753367, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.001297684278525228}. Best is trial 25 with value: 0.016573251293832972.
slurmstepd-csl14c248: error: _handle_stat_jobacct: Took usec=-7357529, which is more than MessageTimeout (10s). The result won't be delivered
[I 2025-09-20 01:55:10,688] Trial 36 pruned. 
[I 2025-09-20 02:02:09,759] Trial 37 pruned. 
[I 2025-09-20 02:05:44,211] Trial 38 pruned. 
[I 2025-09-20 02:09:14,670] Trial 39 pruned. 
[I 2025-09-20 02:24:53,719] Trial 40 pruned. 
[I 2025-09-20 02:52:57,406] Trial 41 finished with value: 0.01719057829408296 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.2801841229081851, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0006068999447472861}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 03:21:04,232] Trial 42 finished with value: 0.016900900256787907 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.2887171728782568, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0012841747382686744}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 03:35:07,153] Trial 43 finished with value: 0.018730703800723985 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.2927926965303505, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0013341277558739362}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 04:03:11,329] Trial 44 finished with value: 0.017018689266165142 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.22202329248650132, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0007433632290007053}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 04:06:41,808] Trial 45 pruned. 
[I 2025-09-20 04:20:44,450] Trial 46 finished with value: 0.01870904727397358 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.26338562060044196, 'downsample_type': 'standard', 'channel_mult': '1124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'mse', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.003306564922380368}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 04:27:43,500] Trial 47 pruned. 
[I 2025-09-20 04:31:13,974] Trial 48 pruned. 
[I 2025-09-20 04:34:48,412] Trial 49 pruned. 
[I 2025-09-20 04:37:22,844] Trial 50 pruned. 
[I 2025-09-20 04:48:19,062] Trial 51 pruned. 
[I 2025-09-20 05:19:33,996] Trial 52 finished with value: 0.01874067172857708 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.18872511123585478, 'downsample_type': 'standard', 'channel_mult': '124', 'attn_config': 'last_two', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.001436335518562217}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 05:23:04,460] Trial 53 pruned. 
[I 2025-09-20 05:34:00,243] Trial 54 pruned. 
[I 2025-09-20 05:37:55,594] Trial 55 pruned. 
[I 2025-09-20 06:05:51,480] Trial 56 pruned. 
[I 2025-09-20 06:37:14,441] Trial 57 finished with value: 0.017166448871862014 and parameters: {'model_channels': 64, 'num_blocks': 2, 'dropout': 0.261616418458583, 'downsample_type': 'standard', 'channel_mult': '1224', 'attn_config': 'last', 'timesteps': 1000, 'beta_schedule': 'linear', 'loss': 'l1', 'optimizer': 'AdamW', 'scheduler': 'WarmupCosine', 'lr': 0.0008365394794223046}. Best is trial 25 with value: 0.016573251293832972.
[I 2025-09-20 06:41:46,208] Trial 58 pruned. 
[I 2025-09-20 06:45:36,514] Trial 59 pruned. 
/home/merlinho/testspace/DDPM_Inpainting/src/run/run_optuna.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.save(torch.load(best_trial.user_attrs["checkpoint"]), model_filename)
